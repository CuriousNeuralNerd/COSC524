{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4f8bf-47e5-4c76-9767-f163c6cc4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Define the maximum sequence length\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# Load NER model for preprocessing\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=device)\n",
    "\n",
    "# NER preprocessing function\n",
    "def extract_entities_and_anonymize(text):\n",
    "    ner_results = ner_pipeline(text)\n",
    "    extracted_entities = []\n",
    "\n",
    "    # Replace entities with their labels\n",
    "    for result in ner_results:\n",
    "        start, end = result['start'], result['end']\n",
    "        entity = result['word']\n",
    "        label = result['entity']\n",
    "        extracted_entities.append(entity)\n",
    "        text = text[:start] + f\"[{label}] \" + text[end:]\n",
    "    \n",
    "    return text, extracted_entities\n",
    "\n",
    "# Function to process a single text file and return sequences with labels for a specific tokenizer\n",
    "def process_file(file_path, label, tokenizer):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Apply NER anonymization\n",
    "    text, _ = extract_entities_and_anonymize(text)\n",
    "\n",
    "    # Tokenize the entire text\n",
    "    tokens = tokenizer(text, add_special_tokens=False)\n",
    "    input_ids = tokens['input_ids']\n",
    "\n",
    "    # Split the input IDs into chunks of MAX_SEQ_LENGTH\n",
    "    chunks = [input_ids[i:i + MAX_SEQ_LENGTH] for i in range(0, len(input_ids), MAX_SEQ_LENGTH)]\n",
    "\n",
    "    # Attach label and file name to each sequence\n",
    "    labeled_sequences = [(chunk, label, file_path) for chunk in chunks]\n",
    "    \n",
    "    return labeled_sequences\n",
    "\n",
    "# Function to process multiple files for a specific category\n",
    "def process_category(file_paths, label, tokenizer):\n",
    "    all_sequences = []\n",
    "    file_sequence_counts = {}\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        labeled_sequences = process_file(file_path, label, tokenizer)\n",
    "        file_sequence_counts[file_path] = len(labeled_sequences)\n",
    "        all_sequences.extend(labeled_sequences)\n",
    "    \n",
    "    return all_sequences, file_sequence_counts\n",
    "\n",
    "# Function to convert sequences into a DataFrame\n",
    "def sequences_to_dataframe(sequences, tokenizer):\n",
    "    texts = [tokenizer.decode(seq[0]) for seq in sequences]\n",
    "    labels = [seq[1] for seq in sequences]\n",
    "    return pd.DataFrame({'content': texts, 'category': labels})\n",
    "\n",
    "# Specify file paths for Train, Test, and Valid sets\n",
    "test_files_0 = [\"./data/train/0_The_Hound_of_the_Baskervilles_CD.txt\"]\n",
    "test_files_1 = [\"./data/test/1_POIROT_INVESTIGATES.txt\"]\n",
    "\n",
    "valid_files_0 = [\"./data/train/0_The_Man_Who_Was_Thursday_GKC.txt\"]\n",
    "valid_files_1 = [\"./data/train/1_THE_BIG_FOUR.txt\"]\n",
    "\n",
    "train_files_0 = [\n",
    "    \"./data/train/0_The_Mystery_of_the_Yellow_Room_GL.txt\",\n",
    "    \"./data/train/0_The_Middle_Temple_Murder_JF.txt\",\n",
    "    \"./data/train/0_JOHN_THORNDYKE'S_CASES_RF.txt\",\n",
    "    \"./data/train/0_A_Study_in_Scarlet_CD.txt\",\n",
    "    \"./data/train/0_Tremendous_Trifles_GKC.txt\",\n",
    "    \"./data/test/0_Caught_in_the_Net_EG.txt\",\n",
    "    \"./data/valid/0_The_Red_House_Mystery_AM.txt\"\n",
    "]\n",
    "\n",
    "train_files_1 = [\n",
    "    \"./data/train/1_The_Secret_of_Chimneys.txt\",\n",
    "    \"./data/train/1_The_Mystery_of_the_Blue_Train.txt\",\n",
    "    \"./data/train/1_The_Mysterious_Affair_at_Styles.txt\",\n",
    "    \"./data/train/1_The_Murder_on_the_Links.txt\",\n",
    "    \"./data/train/1_The_Murder_of_Roger_Ackroyd.txt\",\n",
    "    \"./data/train/1_THE_MAN_IN_THE_BROWN_SUIT.txt\",\n",
    "    \"./data/valid/1_THE_SECRET_ADVERSARY.txt\"\n",
    "]\n",
    "\n",
    "# Tokenizers for each model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "xlm_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare data for each model\n",
    "def prepare_data(tokenizer):\n",
    "    test_sequences = process_category(test_files_0, label=0, tokenizer=tokenizer)[0] + \\\n",
    "                     process_category(test_files_1, label=1, tokenizer=tokenizer)[0]\n",
    "    valid_sequences = process_category(valid_files_0, label=0, tokenizer=tokenizer)[0] + \\\n",
    "                      process_category(valid_files_1, label=1, tokenizer=tokenizer)[0]\n",
    "    train_sequences = process_category(train_files_0, label=0, tokenizer=tokenizer)[0] + \\\n",
    "                      process_category(train_files_1, label=1, tokenizer=tokenizer)[0]\n",
    "\n",
    "    train_data = sequences_to_dataframe(train_sequences, tokenizer)\n",
    "    valid_data = sequences_to_dataframe(valid_sequences, tokenizer)\n",
    "    test_data = sequences_to_dataframe(test_sequences, tokenizer)\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "# Generate datasets for each model\n",
    "train_data_bert, valid_data_bert, test_data_bert = prepare_data(bert_tokenizer)\n",
    "train_data_distilbert, valid_data_distilbert, test_data_distilbert = prepare_data(distilbert_tokenizer)\n",
    "train_data_xlm, valid_data_xlm, test_data_xlm = prepare_data(xlm_tokenizer)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_texts(texts, tokenizer, max_length=MAX_SEQ_LENGTH):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize for BERT\n",
    "bert_train_encodings = tokenize_texts(train_data_bert['content'], bert_tokenizer)\n",
    "bert_val_encodings = tokenize_texts(valid_data_bert['content'], bert_tokenizer)\n",
    "bert_test_encodings = tokenize_texts(test_data_bert['content'], bert_tokenizer)\n",
    "\n",
    "# Tokenize for DistilBERT\n",
    "distilbert_train_encodings = tokenize_texts(train_data_distilbert['content'], distilbert_tokenizer)\n",
    "distilbert_val_encodings = tokenize_texts(valid_data_distilbert['content'], distilbert_tokenizer)\n",
    "distilbert_test_encodings = tokenize_texts(test_data_distilbert['content'], distilbert_tokenizer)\n",
    "\n",
    "# Tokenize for XLM-RoBERTa\n",
    "xlm_train_encodings = tokenize_texts(train_data_xlm['content'], xlm_tokenizer)\n",
    "xlm_val_encodings = tokenize_texts(valid_data_xlm['content'], xlm_tokenizer)\n",
    "xlm_test_encodings = tokenize_texts(test_data_xlm['content'], xlm_tokenizer)\n",
    "\n",
    "# Prepare Dataset for PyTorch\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch datasets for BERT\n",
    "bert_train_dataset = CustomDataset(bert_train_encodings, train_data_bert['category'])\n",
    "bert_val_dataset = CustomDataset(bert_val_encodings, valid_data_bert['category'])\n",
    "bert_test_dataset = CustomDataset(bert_test_encodings, test_data_bert['category'])\n",
    "\n",
    "# Create PyTorch datasets for DistilBERT\n",
    "distilbert_train_dataset = CustomDataset(distilbert_train_encodings, train_data_distilbert['category'])\n",
    "distilbert_val_dataset = CustomDataset(distilbert_val_encodings, valid_data_distilbert['category'])\n",
    "distilbert_test_dataset = CustomDataset(distilbert_test_encodings, test_data_distilbert['category'])\n",
    "\n",
    "# Create PyTorch datasets for XLM-RoBERTa\n",
    "xlm_train_dataset = CustomDataset(xlm_train_encodings, train_data_xlm['category'])\n",
    "xlm_val_dataset = CustomDataset(xlm_val_encodings, valid_data_xlm['category'])\n",
    "xlm_test_dataset = CustomDataset(xlm_test_encodings, test_data_xlm['category'])\n",
    "\n",
    "# Prepare Dataset for PyTorch\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch datasets for BERT\n",
    "bert_train_dataset = CustomDataset(bert_train_encodings, train_data_bert['category'])\n",
    "bert_val_dataset = CustomDataset(bert_val_encodings, valid_data_bert['category'])\n",
    "bert_test_dataset = CustomDataset(bert_test_encodings, test_data_bert['category'])\n",
    "\n",
    "# Create PyTorch datasets for DistilBERT\n",
    "distilbert_train_dataset = CustomDataset(distilbert_train_encodings, train_data_distilbert['category'])\n",
    "distilbert_val_dataset = CustomDataset(distilbert_val_encodings, valid_data_distilbert['category'])\n",
    "distilbert_test_dataset = CustomDataset(distilbert_test_encodings, test_data_distilbert['category'])\n",
    "\n",
    "# Create PyTorch datasets for XLM-RoBERTa\n",
    "xlm_train_dataset = CustomDataset(xlm_train_encodings, train_data_xlm['category'])\n",
    "xlm_val_dataset = CustomDataset(xlm_val_encodings, valid_data_xlm['category'])\n",
    "xlm_test_dataset = CustomDataset(xlm_test_encodings, test_data_xlm['category'])\n",
    "\n",
    "# Plot metrics (loss and accuracy)\n",
    "def plot_metrics(train_losses, eval_losses, eval_accuracies, model_name, fold):\n",
    "    epochs = range(1, len(eval_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, eval_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name} Loss Per Epoch (Fold {fold + 1})')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, eval_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{model_name} Validation Accuracy Per Epoch (Fold {fold + 1})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def cross_validate(model_class, model_name_or_path, tokenizer, train_data, num_folds=3, model_name=\"Model\"):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    texts, labels = train_data['content'], train_data['category']\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # Prepare fold datasets\n",
    "        train_texts, val_texts = texts.iloc[train_idx], texts.iloc[val_idx]\n",
    "        train_labels, val_labels = labels.iloc[train_idx], labels.iloc[val_idx]\n",
    "        train_encodings = tokenize_texts(train_texts, tokenizer)\n",
    "        val_encodings = tokenize_texts(val_texts, tokenizer)\n",
    "        train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "        val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "\n",
    "        # Initialize model\n",
    "        model = model_class.from_pretrained(model_name_or_path, num_labels=2)\n",
    "\n",
    "        # TrainingArguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results_{model_name}_fold{fold}',\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            logging_strategy='epoch',\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=4,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'./logs_{model_name}_fold{fold}',\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "            acc = accuracy_score(labels, predictions)\n",
    "            return {'eval_accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Extract metrics\n",
    "        train_log = trainer.state.log_history\n",
    "        epoch_logs = [log for log in train_log if 'epoch' in log.keys()]\n",
    "        train_losses = [log['loss'] for log in epoch_logs if 'loss' in log]\n",
    "        eval_losses = [log['eval_loss'] for log in epoch_logs if 'eval_loss' in log]\n",
    "        eval_accuracies = [log['eval_accuracy'] for log in epoch_logs if 'eval_accuracy' in log]\n",
    "\n",
    "        # Plot metrics for this fold\n",
    "        plot_metrics(train_losses, eval_losses, eval_accuracies, model_name, fold)\n",
    "\n",
    "        # Evaluate the model on validation set\n",
    "        eval_results = trainer.evaluate(val_dataset)\n",
    "        fold_metrics.append(eval_results)\n",
    "        print(f\"Metrics for Fold {fold + 1}: {eval_results}\")\n",
    "\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "    print(f\"\\nAverage Metrics Across {num_folds} Folds: {avg_metrics}\")\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44c3f8-e394-4145-bd69-bc525ff95d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate for BERT\n",
    "avg_metrics_bert = cross_validate(\n",
    "    model_class=BertForSequenceClassification,\n",
    "    model_name_or_path='bert-base-uncased',\n",
    "    tokenizer=bert_tokenizer,\n",
    "    train_data=train_data_bert,\n",
    "    num_folds=3,\n",
    "    model_name=\"BERT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fc80f-38da-451f-a1e5-b5d18aec04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate for DistilBERT\n",
    "avg_metrics_distilbert = cross_validate(\n",
    "    model_class=DistilBertForSequenceClassification,\n",
    "    model_name_or_path='distilbert-base-cased',\n",
    "    tokenizer=distilbert_tokenizer,\n",
    "    train_data=train_data_distilbert,\n",
    "    num_folds=3,\n",
    "    model_name=\"DistilBERT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e4c88-3ecf-4e64-9c9c-b63cb1a06a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate for XLM-RoBERTa\n",
    "avg_metrics_xlm = cross_validate(\n",
    "    model_class=AutoModelForSequenceClassification,\n",
    "    model_name_or_path='xlm-roberta-base',\n",
    "    tokenizer=xlm_tokenizer,\n",
    "    train_data=train_data_xlm,\n",
    "    num_folds=3,\n",
    "    model_name=\"XLM-RoBERTa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff07119-12f7-45e7-b4df-fb0e601f1cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
